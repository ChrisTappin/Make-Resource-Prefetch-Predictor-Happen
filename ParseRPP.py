import sqlite3
from supporting import RPPO_pb2 # This is one of the files generated by the protobuf module using the .proto file in the Chromium source
from supporting import RPPHR_pb2 # ...and this is the other.
import datetime
import argparse
import os
import sys

csv_help_text = """Enable CSV mode (disabled by default).\n\
  Enabled:  Prints the host and last accessed timestamp in each row, and doesn't covert timstamps to human readable values.\n\
            Prefix and suffix are ignored.\n\
  Disabled: Prints the host and converted timestamp then lists each entry, once per table. Skips records only referring to themselves.\n\
            Output and delimiter are ingnored."""

parser = argparse.ArgumentParser(description='Parse Resource Prefetch Predictor data from Network Action Predictor databases.',formatter_class=argparse.RawTextHelpFormatter)
parser.add_argument("-i", "--input", default="Network Action Predictor", help="Database file to parse. Defaults to 'Network Action Predictor'.")
parser.add_argument("-p", "--prefix", default="", help="Add text before each entry is printed. Defaults to none.")
parser.add_argument("-s", "--suffix", default="", help="Add text after each entry is printed. Defaults to none.")
parser.add_argument("-c", "--csv", help=csv_help_text, action="store_true")
parser.add_argument("-o", "--output", default=datetime.datetime.now(datetime.timezone.utc).strftime('%Y-%m-%d_%H%M%S.txt'), action="store", \
                    help="Initial portion of file names to write if CSV mode is enabled. Defaults to timestamp in format YYYY-MM-DD_HHMMSS.")
parser.add_argument("-d", "--delimiter", default=",", help="Change the CSV delimiter, e.g. \'\\t\' for tab. Defaults to comma.")

args = parser.parse_args()

# sqlite3 will create a file if you ask it to open a DB that doesn't exist.
if not os.path.isfile(args.input):
    print(f"{args.input} not found. Specify the DB file with -i or --input.")
    sys.exit(1)

sqlite_db_file = args.input

csv_mode = args.csv

#if user didn't quote \t, swap for an actual tabulation character
if args.delimiter == '\\t':
    csv_delimiter = '\t'   
else:
    csv_delimiter = args.delimiter

print_prefix = args.prefix
print_suffix = args.suffix

def ListRPPO(RPPOblob):
    if csv_mode == True:
        with open(f"{args.output}.RPPO.csv", 'a', encoding="utf-8") as csv:
            for i in RPPOblob.origins:
                csv.write(f"{RPPO.host}{csv_delimiter}{RPPO.last_visit_time}{csv_delimiter}{i.origin}\n")
    else:
        #skip printing entries where there's only one origin and it matches the host
        if strip_url(RPPOblob.origins[0].origin) == RPPOblob.host and len(RPPOblob.origins) == 1:
            return  
        print(f"OriginStats for {RPPO.host} (last visited {parse_webkit_timestamp(RPPO.last_visit_time)})")
        for i in RPPOblob.origins:
            print(f"{print_prefix}{i.origin}{print_suffix}")

def ListRPPHR(RPPHRblob):
    if csv_mode == True:
        with open(f"{args.output}.RPPHR.csv", 'a', encoding="utf-8") as csv:
            for i in RPPHRblob.redirect_endpoints:
                csv.write(f"{RPPHR.primary_key}{csv_delimiter}{RPPHR.last_visit_time}{csv_delimiter}{i.url}\n")
    else:
        # skip printing entries where there's only one url and it matches the primary key
        if RPPHRblob.redirect_endpoints[0].url == RPPHR.primary_key and len(RPPHRblob.redirect_endpoints) == 1:
            return            
        print(f"RedirectStats for {RPPHR.primary_key} (last visited {parse_webkit_timestamp(RPPHR.last_visit_time)})")
        for i in RPPHRblob.redirect_endpoints:
            print(f"{print_prefix}{i.url}{print_suffix}")

def parse_webkit_timestamp(timestamp):
    time = datetime.timedelta(microseconds=int(timestamp))
    time = datetime.datetime(1601,1,1) + time
    return (time)

def fetch_sqlite_records(table='resource_prefetch_predictor_origin',sqlite_db_file='Network Action Predictor'):
    con = sqlite3.connect(sqlite_db_file)
    cur = con.cursor()
    res = cur.execute(f"SELECT * FROM {table}")
    records = res.fetchall()
    return records

def strip_url(url):
    url = url.split('/')[2]
    return url

if csv_mode == True:
    # Open the file for writing
    with open(f"{args.output}.RPPO.csv", 'a', encoding="utf-8") as csv:
        # Print a header row if we're making a CSV
        csv.write(f"host{csv_delimiter}last_visit_time{csv_delimiter}origin\n")

# Open each RPPO record and parse the blob
RPPO = RPPO_pb2.OriginData()
RPPOrecords = fetch_sqlite_records('resource_prefetch_predictor_origin', sqlite_db_file)

for record in RPPOrecords:
    RPPO.ParseFromString(record[1])
    ListRPPO(RPPO)

if csv_mode == True:
    # Open the file for writing
    with open(f"{args.output}.RPPHR.csv", 'a', encoding="utf-8") as csv:
        # Print a header row if we're making a CSV
        csv.write(f"primary_key{csv_delimiter}last_visit_time{csv_delimiter}url\n")

# Open each RPPHR record and parse the blob
RPPHR = RPPHR_pb2.RedirectData()
RPPHRrecords = fetch_sqlite_records('resource_prefetch_predictor_host_redirect', sqlite_db_file)

for record in RPPHRrecords:
    RPPHR.ParseFromString(record[1])
    ListRPPHR(RPPHR)

